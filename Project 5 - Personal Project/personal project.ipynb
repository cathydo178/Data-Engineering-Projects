{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pyspark.sql.functions import udf, expr, first, col\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "I am going to create a STAR schema for this database. \n",
    "\n",
    "*1 fact table* \\\n",
    "*1 i94 immigration dimension table* \\\n",
    "*1 temperature dimension table*\n",
    "*1 airport code dimension table* \\\n",
    "*1 US cities demographics table* \n",
    "\n",
    "Immigration data and temperature data will be aggregated by city. Airport code and US cities demographics data will be aggregated by state. I'm using Spark to build my ETL pipeline and process data into analytic tables. \n",
    "\n",
    "My final dataset will help the analytic team draw insights on immigration behaviors. \n",
    "\n",
    "#### Describe and Gather Data \n",
    "For this project, I pick the 4 following datasets:\n",
    "* I94 Immigration Data: This data comes from the US National Tourism and Trade Office. A data dictionary is included in the notebook. [This](https://travel.trade.gov/research/reports/i94/historical/2016.html) is where the data comes from.\n",
    "* World Temperature Data: This dataset come from Kaggle. [This](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data) is where the data comes from.\n",
    "* U.S. City Demographic Data: This dataset comes from OpenSoft. [This](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/) is where the data comes from.\n",
    "* Airport Code Table: This is a simple table of airport codes and corresponding cities. [This](https://datahub.io/core/airport-codes#data) is where the data comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read Immigration data for the month of April, 2016\n",
    "i94_immigration = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "i94_df = pd.read_sas(i94_immigration, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i94_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read Temperature data\n",
    "temperature = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "temp_df = pd.read_csv(temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read Cities Demographics data\n",
    "demographics = 'us-cities-demographics.csv'\n",
    "demographics_df = pd.read_csv(demographics, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read airport code data\n",
    "airport = 'airport-codes_csv.csv'\n",
    "airport_df = pd.read_csv(airport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Creating Connection to Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "#df_spark.write.parquet(\"sas_data\")\n",
    "#df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['XXX', 'ATL', 'WAS', 'NYC', 'TOR', 'BOS', 'HOU', 'MIA', 'CHI',\n",
       "       'LOS', 'CLT', 'DEN', 'DAL', 'DET', 'NEW', 'FTL', 'LVG', 'ORL',\n",
       "       'NOL', 'PIT', 'SFR', 'SPM', 'POO', 'PHI', 'SEA', 'SLC', 'TAM',\n",
       "       'HAM', 'NAS', 'VCV', 'MAA', 'AUS', 'HHW', 'OGG', 'PHO', 'SDP',\n",
       "       'SFB', 'EDA', 'MON', 'CLG', 'DUB', 'FMY', 'YGF', 'SAJ', 'CIN',\n",
       "       'BAL', 'RDU', 'WPB', 'STT', 'OAK', 'NSV', 'SNA', 'OTT', 'X96',\n",
       "       '5KE', 'CLE', 'HAR', 'PSP', 'CHR', 'HAL', 'SAA', 'KOA', 'SHA',\n",
       "       'WIN', 'BGM', 'NCA', 'OPF', 'SAI', 'JFA', 'AGA', 'ONT', 'CLM',\n",
       "       'STL', 'W55', 'CHS', 'SNJ', 'SRQ', 'ANC', 'LNB', 'LIH', 'MIL',\n",
       "       'INP', 'KAN', 'ROC', 'SAC', 'BRO', 'LAR', 'RNO', 'SGR', 'ELP',\n",
       "       'MCA', 'MDT', 'SPE', 'FPR', 'SYR', 'ICT', 'MLB', 'ADS', 'TUC',\n",
       "       'DLR', 'CAE', 'CHA', 'HSV', 'WIL', 'HPN', 'HEF', 'BRG', 'BED',\n",
       "       'DAB', 'JAC', 'FRB', 'SWF', 'KEY', 'PTK', 'MWH', 'X44', 'MYR',\n",
       "       'APF', 'ATW', 'PVD', 'BUF', 'PIE', 'MHT', 'BDL', 'NYL', 'VNY',\n",
       "       '5T6', 'LEX', 'NOR', 'BQN', 'MEM', 'INT', 'CRQ', 'SPO', 'FOK',\n",
       "       'PEV', 'FAR', 'MAF', 'TKI', 'OMA', 'LOU', 'PHF', 'RST', 'MMU',\n",
       "       'CPX', 'SCH', 'RYY', 'PEM', 'JKM', 'LYN', 'OGD', 'NC8', 'MOB',\n",
       "       'SAV', 'HIG', 'CHL', 'WLL', 'MTH', 'AXB', 'SUM', 'ADW', 'SGJ',\n",
       "       'JMZ', 'BLA', 'SSM', 'YIP', 'EPI', 'GSP', 'BHX', 'MND', 'FCA',\n",
       "       'CRP', 'YHC', 'PHU', 'COB', 'OTM', 'STR', 'PSM', 'FWA', 'SYS',\n",
       "       'PEN', 'ABQ', 'HEL', 'DPA', 'CHM', 'EGP', 'POR', 'PIR', 'ORO',\n",
       "       'LUK', 'DER', 'DOU', 'SWE', 'NOO', 'LAN', 'VIC', 'COO', 'NRN',\n",
       "       'REN', 'HTM', 'HID', 'BEL', 'CLS', 'PRO', 'PRE', 'PCF', 'RIF',\n",
       "       'ROS', 'PAR', 'BEE', 'DAC', 'NOG', 'BTN', 'DNS', 'NAC', 'GAL',\n",
       "       'FPT', 'ABG', 'MAS', 'PTL', 'TEC', 'ROO', 'BAU', 'FRI', 'TRO',\n",
       "       'ANA', 'FRE', 'AND', 'LEW', 'NIA', 'PBB', 'THO', 'YSL', 'BEB',\n",
       "       'DLB', 'DNA', 'FTC', 'GPM', 'LAU', 'LCB', 'LLB', 'MOO', 'NEC',\n",
       "       'PHR', 'ROU', 'SKA', 'WHO', 'ANZ', 'BOA', 'CAL', 'MAD', 'MOR',\n",
       "       'ROM', 'WBE', 'AGN', 'CNA', 'SLU', 'FER', 'ALC', 'MET', 'PDN',\n",
       "       'PNH', 'VIB', 'WAL', 'BWA', 'CHT', 'DVL', 'FRT', 'HNN', 'HNS',\n",
       "       'HVR', 'SJO', 'WAR', 'COL', 'TUR', 'ABS', 'BWM', 'CNC', 'RAY',\n",
       "       'VCB', 'MGM', 'MRC', 'PGR', 'LOI', 'ADT', 'NRG', 'CRY', 'ERC',\n",
       "       'FTF', 'FTK', 'SHR', 'MAI', 'NIG', 'NRT', 'VNB', 'FAL', 'LUB',\n",
       "       'RIO', 'LWT'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring data sets\n",
    "    # Immigration Data\n",
    "\n",
    "i94_df.i94port.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.068,    nan,  5.788, ...,  9.202,  6.875,  6.66 ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Temperature Data\n",
    "temp_df.AverageTemperature.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hispanic or Latino                   596\n",
       "White                                589\n",
       "Black or African-American            584\n",
       "Asian                                583\n",
       "American Indian and Alaska Native    539\n",
       "Name: Race, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Demographic data\n",
    "    #Race count\n",
    "demographics_df.Race.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Silver Spring', 'Quincy', 'Hoover', 'Rancho Cucamonga', 'Newark',\n",
       "       'Peoria', 'Avondale', 'West Covina', \"O'Fallon\", 'High Point',\n",
       "       'Folsom', 'Philadelphia', 'Wichita', 'Fort Myers', 'Pittsburgh',\n",
       "       'Laredo', 'Berkeley', 'Santa Clara', 'Allen', 'Hampton',\n",
       "       'Bolingbrook', 'Frederick', 'Sparks', 'Rancho Cordova',\n",
       "       'Westminster', 'Lakewood', 'Flint', 'New Haven', 'Brooklyn Park',\n",
       "       'Chula Vista', 'Danbury', 'Framingham', 'Saint Petersburg',\n",
       "       'Miami Gardens', 'Salt Lake City', 'Suffolk', 'North Little Rock',\n",
       "       'Jurupa Valley', 'Los Angeles', 'Flower Mound', 'Vacaville',\n",
       "       'Clarksville', 'New Britain', 'Tulsa', 'Seattle', 'Mesa', 'Yonkers',\n",
       "       'Camden', 'Alexandria', 'Jonesboro', 'El Monte', 'Roswell', 'Omaha',\n",
       "       'Troy', 'Winston-Salem', 'Corpus Christi', 'Atlanta', 'Bryan',\n",
       "       'Lynchburg', 'Columbia', 'Killeen', 'Nashville', 'Somerville',\n",
       "       'Bakersfield', 'Montgomery', 'Huntsville', 'Greensboro', 'Turlock',\n",
       "       'Lynwood', 'El Paso', 'Lexington-Fayette  county', 'Rock Hill',\n",
       "       'Richardson', 'Waco', 'San Buenaventura', 'Stamford', 'Vallejo',\n",
       "       'Denver', 'Springdale', 'Chino', 'Toms River', 'Lynn', 'Lawton',\n",
       "       'Garland', 'Layton', 'Bloomington', 'Newton', 'Clearwater',\n",
       "       'Kenner', 'Las Cruces', 'Yakima', 'Des Moines', 'Wichita Falls',\n",
       "       'Wilmington', 'Cranston', 'Amarillo', 'Buffalo', 'Livonia',\n",
       "       'San Leandro', 'Fresno', 'Saint George', 'San Juan', 'Redlands',\n",
       "       'Thousand Oaks', 'Rio Rancho', 'Provo', 'Tallahassee', 'Aurora',\n",
       "       'Minneapolis', 'Anaheim', 'San Marcos', 'Norwalk', 'Fort Worth',\n",
       "       'Cambridge', 'Deerfield Beach', 'Beaumont', 'Spring Valley',\n",
       "       'Sioux City', 'Alafaya', 'Washington', 'Grand Prairie', 'Waukesha',\n",
       "       'Miami Beach', 'North Las Vegas', 'Fort Smith', 'Salinas',\n",
       "       'Springfield', 'Bellingham', 'Green Bay', 'Maple Grove',\n",
       "       'Escondido', 'Rockford', 'Memphis', 'Caguas', 'Metairie', 'Tracy',\n",
       "       'Champaign', 'West Palm Beach', 'Beaverton', 'Ogden', 'Decatur',\n",
       "       'Pompano Beach', 'Nashua', 'Vancouver', 'Fargo', 'El Cajon',\n",
       "       'Lancaster', 'Miramar', 'Riverview', 'Arlington Heights', 'Lorain',\n",
       "       'Mesquite', 'Appleton', 'Johns Creek', 'Nampa', 'Sterling Heights',\n",
       "       'Tacoma', 'Manchester', 'Overland Park', 'Cicero', 'Weston',\n",
       "       'Meridian', 'Trenton', 'Temecula', 'Saint Charles', 'Antioch',\n",
       "       'Eugene', 'Edinburg', 'Huntington Beach', 'Torrance', 'South Bend',\n",
       "       'Bethlehem', 'Birmingham', 'Hawthorne', 'Boca Raton', 'Vista',\n",
       "       'Brockton', 'New Rochelle', 'Dearborn', 'Charleston', 'Elizabeth',\n",
       "       'Syracuse', 'Redding', 'Scottsdale', 'Glendale', 'Fishers',\n",
       "       'Waldorf', 'Las Vegas', 'Victoria', 'Anchorage', 'San Diego',\n",
       "       'Dayton', 'Auburn', 'Charlotte', 'Gulfport', 'Abilene',\n",
       "       'Centreville', 'Pleasanton', 'Waterbury', 'Davie', 'Houston',\n",
       "       'Concord', 'Eagan', 'Duluth', 'Carolina', 'Ellicott City',\n",
       "       'Port Saint Lucie', 'Greeley', 'Ames', 'West Jordan', 'Chico',\n",
       "       'Chino Hills', 'New Bedford', 'San Bernardino', 'Longmont',\n",
       "       'Lincoln', 'Pasco', 'Cincinnati', 'New Braunfels', 'Union City',\n",
       "       'Fontana', 'Milwaukee', 'Racine', 'Santa Maria', 'Marysville',\n",
       "       'Inglewood', 'Cedar Rapids', 'German', 'Highlands Ranch', 'Tustin',\n",
       "       'Kent', 'Urban Honolulu', 'Richmond', 'Deltona', 'Kissimmee',\n",
       "       'San Mateo', 'Fall River', 'Carmichael', 'Lauderhill', 'Worcester',\n",
       "       'Providence', 'Fayetteville', 'Santa Rosa', 'Lakeland', 'Boulder',\n",
       "       'Bayonne', 'Santa Barbara', 'Yuba City', 'Palo Alto', 'Pueblo',\n",
       "       'The Villages', 'Fort Wayne', 'Santa Ana', 'Bismarck', 'Phoenix',\n",
       "       'Daly City', 'South San Francisco', 'Independence', 'Kennewick',\n",
       "       'Flagstaff', 'Brandon', 'Kansas City', 'Waukegan', 'Fort Collins',\n",
       "       'Jacksonville', 'Cary', 'Saint Joseph', 'Melbourne', 'Palm Bay',\n",
       "       'Plantation', 'Olathe', 'Sioux Falls', 'Portsmouth', 'Jackson',\n",
       "       'Spokane', 'Colorado Springs', 'McKinney', 'Henderson', 'Norfolk',\n",
       "       'Arvada', 'Hollywood', 'Chesapeake', 'Columbus', 'Spring Hill',\n",
       "       'Kenosha', 'Kendall', 'Redondo Beach', 'Walnut Creek', 'Mission',\n",
       "       'McAllen', 'Cheektowaga', 'Oceanside', 'Orlando', 'Parma',\n",
       "       'Harlingen', 'Pembroke Pines', 'Alhambra', 'Brentwood', 'Riverside',\n",
       "       'Menifee', 'Kalamazoo', 'Carson', 'Ann Arbor', 'Farmington Hills',\n",
       "       'Missouri City', 'Orange', 'Everett', 'Citrus Heights',\n",
       "       'Gainesville', 'Milpitas', 'Tampa', 'Santa Monica', 'Eau Claire',\n",
       "       'Enterprise', 'Shawnee', 'North Charleston', 'Boynton Beach',\n",
       "       'Temple', 'Rockville', 'Cape Coral', 'Manteca', 'Little Rock',\n",
       "       'Oxnard', 'Bk', 'Evanston', 'Atascocita', 'Surprise', 'Dothan',\n",
       "       'Rapid City', \"Lee's Summit\", 'Warren', 'Erie', 'Mountain View',\n",
       "       'West Valley City', 'Hartford', 'Fremont', 'Elgin', 'Greenville',\n",
       "       'Gastonia', 'Frisco', 'Oklahoma City', 'Carlsbad', 'Pasadena',\n",
       "       'Naperville', 'Johnson City', 'Goodyear', 'Wyoming', 'Reading',\n",
       "       'Rochester Hills', 'Madison', 'Portland', 'College Station',\n",
       "       'Lewisville', 'Raleigh', 'Mount Vernon', 'Dallas', 'San Tan Valley',\n",
       "       'East Orange', 'Austin', 'Casas Adobes', 'Indianapolis', 'Renton',\n",
       "       'Odessa', 'Round Rock', 'Santa Fe', 'Akron', 'San Jose',\n",
       "       'Saint Paul', 'Muncie', 'Victorville', 'Murfreesboro',\n",
       "       'Chattanooga', 'Fairfield', 'Toledo', 'Asheville', 'Savannah',\n",
       "       'Louisville/Jefferson County metro government', 'Yorba Linda',\n",
       "       'Billings', 'Denton', 'Long Beach', 'Tuscaloosa', 'Elk Grove',\n",
       "       'Paradise', 'Sandy Springs', 'Pearland', 'Virginia Beach',\n",
       "       'Roseville', 'Homestead', 'Iowa City', 'Arden-Arcade', 'Largo',\n",
       "       'Carrollton', 'Chandler', 'Irvine', 'Medford', 'Coral Springs',\n",
       "       'Bossier City', 'Woodbury', 'Boston', 'Pittsburg', 'San Ramon',\n",
       "       'Reno', 'Topeka', 'Newport News', 'Poinciana', 'Compton',\n",
       "       'Cedar Park', 'Merced', 'Shreveport', 'Palatine', 'San Angelo',\n",
       "       'Passaic', 'Lake Charles', 'North Richland Hills', 'Boise',\n",
       "       'Carmel', 'Palmdale', 'Baltimore', 'Indio', 'Florence-Graham',\n",
       "       'Santa Clarita', 'Buena Park', 'San Clemente', 'Lawrence', 'Downey',\n",
       "       'Lafayette', 'Newport Beach', 'Schenectady', 'South Gate',\n",
       "       'Brownsville', 'Costa Mesa', 'Tyler', 'Napa', 'Centennial',\n",
       "       'Sunrise', 'Tucson', 'Fullerton', 'Lake Forest', 'Stockton',\n",
       "       'San Antonio', 'Orem', 'Norman', 'Youngs', 'Hayward', 'Sandy',\n",
       "       'Moreno Valley', 'Delray Beach', 'Gresham', 'Federal Way',\n",
       "       'Waterloo', 'Jersey City', 'Mission Viejo', 'Dale City',\n",
       "       'Albuquerque', 'Plano', 'Hesperia', 'Lehigh Acres', 'Pawtucket',\n",
       "       \"Town 'n' Country\", 'Saint Cloud', 'Midland', 'Chicago',\n",
       "       'Knoxville', 'Westland', 'Yuma', 'Roanoke', 'Rialto', 'Pomona',\n",
       "       'Warner Robins', 'East Los Angeles', 'Simi Valley', 'Davenport',\n",
       "       'Thornton', 'Gary', 'Palm Coast', 'Hialeah', 'Clifton',\n",
       "       'Bellflower', 'Broomfield', 'Joliet', 'San Francisco', 'Camarillo',\n",
       "       'Skokie', 'Redwood City', 'Evansville', 'Hillsboro',\n",
       "       'The Woodlands', 'Longview', 'Upland', 'Sunrise Manor', 'Lubbock',\n",
       "       'Kirkland', 'Garden Grove', 'Baton Rouge', 'Livermore', 'Paterson',\n",
       "       'Conroe', 'Edmond', 'Sacramento', 'Detroit', 'Ontario', 'Alameda',\n",
       "       'Clovis', 'Gilbert', 'Hammond', 'Saint Louis',\n",
       "       'Athens-Clarke County unified government', 'Scranton', 'Modesto',\n",
       "       'Perris', 'South Jordan', 'Visalia', 'Davis', 'Gaithersburg',\n",
       "       'Albany', 'Plymouth', 'Corona', 'Rochester', 'Oshkosh',\n",
       "       'Baldwin Park', 'Oakland',\n",
       "       'Augusta-Richmond County consolidated government', 'Salem',\n",
       "       'Schaumburg', 'Lowell', 'Glen Burnie', 'Bend', 'Bridgeport',\n",
       "       'Broken Arrow', 'New Orleans', 'Arlington', 'Bellevue', 'Irving',\n",
       "       'Whittier', 'Bay', 'Loveland', 'Mount Pleasant', 'Hemet',\n",
       "       'Franklin', 'Sugar Land', 'Murrieta', 'Miami', 'Apple Valley',\n",
       "       'Macon', 'Cleveland', 'Tempe', 'Southfield', 'Durham', 'Pharr',\n",
       "       'Spokane Valley', 'Grand Rapids', 'Sunnyvale', 'Fort Lauderdale',\n",
       "       'Warwick', 'Canton', 'Lansing', 'Mayagüez', 'Laguna Niguel',\n",
       "       'Pine Hills', 'New York', 'Ponce', 'Bayamón', 'Mobile',\n",
       "       'League City', 'Guaynabo', 'Missoula'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    #Unique city\n",
    "demographics_df.City.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "US    22757\n",
       "BR     4334\n",
       "CA     2784\n",
       "AU     1963\n",
       "KR     1376\n",
       "MX     1181\n",
       "RU     1040\n",
       "DE      947\n",
       "GB      911\n",
       "FR      850\n",
       "AR      848\n",
       "CO      706\n",
       "IT      671\n",
       "PG      593\n",
       "VE      592\n",
       "ZA      489\n",
       "CL      474\n",
       "ID      470\n",
       "ES      416\n",
       "CN      404\n",
       "KE      372\n",
       "IN      341\n",
       "CD      285\n",
       "PH      282\n",
       "PL      278\n",
       "CZ      269\n",
       "JP      234\n",
       "NO      228\n",
       "SE      224\n",
       "NZ      212\n",
       "      ...  \n",
       "DM        2\n",
       "SM        2\n",
       "WF        2\n",
       "KN        2\n",
       "RE        2\n",
       "PM        2\n",
       "LC        2\n",
       "MF        2\n",
       "SH        2\n",
       "BN        2\n",
       "AD        2\n",
       "NU        1\n",
       "GM        1\n",
       "BL        1\n",
       "LI        1\n",
       "NR        1\n",
       "MQ        1\n",
       "GI        1\n",
       "SX        1\n",
       "MO        1\n",
       "CC        1\n",
       "AI        1\n",
       "AW        1\n",
       "CX        1\n",
       "VA        1\n",
       "CW        1\n",
       "IO        1\n",
       "JE        1\n",
       "YT        1\n",
       "NF        1\n",
       "Name: iso_country, Length: 243, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Airport code data\n",
    "    #country counts\n",
    "airport_df.iso_country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "small_airport     33965\n",
       "heliport          11287\n",
       "medium_airport     4550\n",
       "closed             3606\n",
       "seaplane_base      1016\n",
       "large_airport       627\n",
       "balloonport          24\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    #airport type counts\n",
    "airport_df.type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "US-TX     2277\n",
       "US-CA     1088\n",
       "US-FL      967\n",
       "US-PA      918\n",
       "BR-SP      907\n",
       "US-IL      902\n",
       "US-AK      829\n",
       "US-OH      799\n",
       "GB-ENG     726\n",
       "US-IN      697\n",
       "CA-ON      695\n",
       "US-NY      668\n",
       "BR-MT      635\n",
       "US-WI      624\n",
       "US-LA      592\n",
       "US-WA      578\n",
       "US-MO      578\n",
       "US-MN      569\n",
       "US-MI      549\n",
       "US-OK      537\n",
       "BR-MS      527\n",
       "US-GA      522\n",
       "AU-QLD     511\n",
       "US-CO      505\n",
       "US-VA      505\n",
       "US-OR      492\n",
       "US-NC      473\n",
       "CA-BC      467\n",
       "US-NJ      442\n",
       "US-KS      439\n",
       "          ... \n",
       "IR-31        1\n",
       "RO-AB        1\n",
       "UG-312       1\n",
       "SI-174       1\n",
       "SI-188       1\n",
       "GD-PA        1\n",
       "EC-A         1\n",
       "MV-01        1\n",
       "TL-CO        1\n",
       "FM-PNI       1\n",
       "SM-06        1\n",
       "AE-SH        1\n",
       "TR-39        1\n",
       "PW-350       1\n",
       "AZ-AGS       1\n",
       "EE-39        1\n",
       "MV-08        1\n",
       "MN-073       1\n",
       "AZ-FUZ       1\n",
       "NG-ED        1\n",
       "JP-10        1\n",
       "TR-53        1\n",
       "TR-76        1\n",
       "MH-LIK       1\n",
       "AZ-XCI       1\n",
       "BD-1         1\n",
       "RS-17        1\n",
       "GE-GU        1\n",
       "MA-ERR       1\n",
       "GR-33        1\n",
       "Name: iso_region, Length: 2810, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    #State counts\n",
    "airport_df.iso_region.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Clean Immigration Data\n",
    "    #Create a valid i94 port file\n",
    "\n",
    "re_obj = re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "i94ports = {}\n",
    "with open('i94port.txt') as i94:\n",
    "    for port in i94:\n",
    "        match = re_obj.search(port)\n",
    "        i94ports[match[1]] = [match[2]]\n",
    "        \n",
    "    #Drop entries where i94port value is invalid such as: XXX,...    \n",
    "df_i94 =spark.read.format('com.github.saurfang.sas.spark').load(i94_immigration)\n",
    "df_i94 = df_i94.filter(df_i94.i94port.isin(list(i94ports.keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Clean temperature Data\n",
    "    #Drop entries where temperature value is NaN\n",
    "df_temperature = spark.read.format('csv').option('header', 'true').load(temperature)\n",
    "df_temperature = df_temperature.filter(df_temperature.AverageTemperature != 'Nan')\n",
    "\n",
    "    #Drop duplicate city and countries\n",
    "df_temperature = df_temperature.dropDuplicates(['City', 'Country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+---------------------------------+------+-------------------------+------------------+------+\n",
      "|                City|State Code|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|American Indian and Alaska Native| Asian|Black or African-American|Hispanic or Latino| White|\n",
      "+--------------------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+---------------------------------+------+-------------------------+------------------+------+\n",
      "|             Lynwood|        CA|      29.4|          35634|            36371|           72005|               776|       28061|                  4.43|                             null|   994|                     5346|             63377| 48670|\n",
      "|           Hollywood|        FL|      41.4|          75358|            74363|          149721|              6056|       55158|                  2.65|                             1010|  4696|                    34916|             53247|107916|\n",
      "|             Fremont|        CA|      38.3|         114383|           117808|          232191|              4629|      109427|                  3.12|                             1606|141953|                     7762|             29594| 67294|\n",
      "|          Atascocita|        TX|      32.8|          37424|            40816|           78240|              4416|        8657|                  3.03|                             1024|  2441|                    21491|             20441| 50630|\n",
      "|             Brandon|        FL|      36.1|          55679|            58289|          113968|              9417|       16390|                  2.64|                             1921|  9440|                    23475|             26224| 80811|\n",
      "|           Rockville|        MD|      38.1|          31205|            35793|           66998|              1990|       25047|                   2.6|                              594| 17370|                     7533|              9197| 41692|\n",
      "|          Rapid City|        SD|      39.5|          36122|            37446|           73568|              7429|        2375|                  2.31|                             9159|  2170|                     2217|              2971| 63983|\n",
      "|              Conroe|        TX|      31.5|          34907|            33685|           68592|              4871|       13839|                  2.65|                              707|  2302|                     6713|             23863| 57480|\n",
      "|       Oklahoma City|        OK|      34.1|         309227|           322036|          631263|             41843|       80165|                  2.58|                            41696| 33759|                   111673|            117362|462306|\n",
      "|             Hayward|        CA|      34.3|          78152|            80148|          158300|              4385|       60970|                  3.32|                             2178| 46285|                    20219|             64880| 74800|\n",
      "|            Franklin|        TN|      38.3|          34991|            37644|           72635|              3303|        6876|                  2.73|                             null|  3911|                     7749|              4916| 61459|\n",
      "|           Sunnyvale|        CA|      35.7|          79480|            72280|          151760|              3900|       71441|                  2.74|                             1179| 69472|                     4210|             25638| 71463|\n",
      "|             Decatur|        IL|      40.4|          34646|            38210|           72856|              5291|        1224|                  2.23|                              265|  1343|                    17939|              1709| 56231|\n",
      "|            Syracuse|        NY|      30.3|          69462|            74690|          144152|              5845|       17733|                  2.39|                             3606|  9386|                    46026|             13368| 88679|\n",
      "|          Des Moines|        IA|      34.5|         103726|           106591|          210317|             11780|       23857|                  2.41|                             3964| 15205|                    26353|             25306|168057|\n",
      "|              Olathe|        KS|      33.0|          65046|            69270|          134316|              6042|       11913|                  2.97|                             1872|  6288|                     9518|             16131|120282|\n",
      "|               Eagan|        MN|      36.8|          31587|            34701|           66288|              2699|        8642|                  2.49|                              684|  6923|                     8032|              4328| 52612|\n",
      "|           Pawtucket|        RI|      39.2|          36072|            35511|           71583|              3173|       17884|                  2.41|                             1359|  2150|                    15513|             16117| 47614|\n",
      "|North Richland Hills|        TX|      39.9|          35257|            33948|           69205|              5175|        7854|                  2.64|                             1127|  5295|                     5439|             10195| 58258|\n",
      "|            Alhambra|        CA|      41.0|          42184|            43388|           85572|              1673|       44441|                  2.89|                              687| 44067|                     1905|             31386| 20811|\n",
      "+--------------------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+---------------------------------+------+-------------------------+------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Clean us-cities-demographics data\n",
    "    #Transform value in 'race' row into columns\n",
    "demographics_df = spark.read.format('csv').option('sep', ';').option('header', 'true').load(demographics)\n",
    "    \n",
    "demographic_race_count = (demographics_df.select('city', 'state code', 'Race', 'Count').groupBy(demographics_df.City, 'state code')\n",
    "                    .pivot('Race').agg(first('Count')))\n",
    "\n",
    "    #Drop duplicates\n",
    "demographics_df= demographics_df.dropDuplicates()\n",
    "\n",
    "    #Drop 'race column' and 'count column' from original dataset\n",
    "demographics_df = demographics_df.drop('Race', 'Count')\n",
    "\n",
    "    #join 2 datasets\n",
    "demographics_final = demographics_df.join(demographic_race_count, ['city', 'state code'])\n",
    "\n",
    "    #drop state column\n",
    "demographics_final = demographics_final.drop('State')\n",
    "\n",
    "demographics_final.show()\n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "##Clean airport code data\n",
    "    #Drop entries where airport type is closed\n",
    "airport_df = spark.read.format('csv').option('header', 'true').load(airport)\n",
    "airport_df = airport_df.filter(airport_df.type != 'closed')\n",
    "    \n",
    "    #Drop entries where iata_code is null\n",
    "airport_df = airport_df.filter(airport_df.iata_code != 'null')\n",
    "    \n",
    "    #Create a new column for state code\n",
    "airport_df = airport_df.withColumn('state', expr('substr(iso_region, 4, length(iso_region))'))\n",
    "\n",
    "    #Drop iso_region column\n",
    "airport_df = airport_df.drop('iso_region')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "(For explanation of code, please see I94_SAS_Labels_Description.SAS file)\n",
    "\n",
    "**FACT TABLE**: quantitative data\n",
    "\n",
    "* i94yr: year\n",
    "* i94mon: month\n",
    "* i94 cit: origin city\n",
    "* i94 port: destination city\n",
    "* i94 mode: travel code\n",
    "* i94 visa: reason for immigration\n",
    "* Average Temperature: teperature\n",
    "* port_state: state code\n",
    "\n",
    "**FIRST DIMENSION TABLE**: immigration data\n",
    "\n",
    "* i94yr: year\n",
    "* i94mon: month\n",
    "* i94cit: city\n",
    "* i94port: destination city\n",
    "* i94mode: travel code\n",
    "* i94bir: age of respondents in years\n",
    "* arrdate: arrival date\n",
    "* depdate: depature date\n",
    "* i94visa: reason for immigration\n",
    "\n",
    "**SECOND DIMENSION TABLE**: temperature data\n",
    "\n",
    "* i94port: destination city\n",
    "* average temperature: temperature\n",
    "* city: city name\n",
    "* country: country name\n",
    "* latitude: latitude\n",
    "* longtitude: longtitude\n",
    "\n",
    "**THIRD DIMENSION TABLE**: airport code data\n",
    "\n",
    "* type: type of airport (small, medium,...)\n",
    "* name: airport name\n",
    "* continent: name of continent where the airport locates\n",
    "* iso_country: country name\n",
    "* state: state code\n",
    "* municipality: town name\n",
    "* local_code: local code\n",
    "* coordinates: coordinates\n",
    "\n",
    "**FOURTH DIMENSION TABLE**: U.S. City Demographics data\n",
    "\n",
    "* City: city name\n",
    "* State: state code\n",
    "* median_age: average age\n",
    "* male_population: male population\n",
    "* female_population: female population\n",
    "* total_population: total population\n",
    "* foreign_born: foreign born\n",
    "* avg_household_size: Average household size\n",
    "* asian: asian race\n",
    "* black: black or African American race\n",
    "* hispanic: hispanic or latino race\n",
    "* white: white race\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Steps necessary to pipeline the data into the chosen data model:\n",
    "\n",
    "* Import and Install all necessary packages.\n",
    "* Read 4 dataset files to dataframe\n",
    "* Explore datasets to identify null, invalid, duplicates data\n",
    "* Clean dataset as described in step 2\n",
    "* Create immigration dimension table by:\n",
    "    1. create a new dataframe df_i94 that include port city and port state\n",
    "    2. Extracting relevant columns from df_i94\n",
    "    3. Write to parquet file partitioned by i94port    \n",
    "* Create temperature dimension table by:\n",
    "    1. Add i94 port column to df_temperature, correspond port code to city name\n",
    "    2. Extracting relevant columns from df_temperature\n",
    "    3. Write to parquet file partitioned by i94port\n",
    "* Create airport code dimension table by:\n",
    "    1. Extract relevant columns from airport_df\n",
    "    2. Write to parquet file partitioned by state\n",
    "* Create U.S Cities Demographics table by:\n",
    "    1. Extract relevant columns from demographics_final\n",
    "    2. Write parquet file partitioned by state\n",
    "* Create fact table by:\n",
    "    1. Create 2 temporary SQL tables: immigration_table and temperature_table\n",
    "    2. Joining immigration_table and temperature_table together\n",
    "    3. Write parquet file partitioned by port_state and port\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read i94port text file\n",
    "i94port_df = pd.read_csv('i94port.txt', sep = '=', names=['symbol','port_name'])\n",
    "\n",
    "#remove whitespaces and single quotes\n",
    "i94port_df['symbol']=i94port_df['symbol'].str.strip().str.replace(\"'\",'')\n",
    "\n",
    "#create two columns from i94port string: port_city, and port_addr\n",
    "i94port_df['port_city'], i94port_df['port_state']=i94port_df['port_name'].str.strip().str.replace(\"'\", '').str.strip().str.split(',',1).str\n",
    "\n",
    "#Remove more whitespace from port_addr\n",
    "i94port_df['port_state']=i94port_df['port_state'].str.strip()\n",
    "\n",
    "#Drop port column\n",
    "i94port_df.drop(columns=['port_name'], inplace = True)\n",
    "\n",
    "#Convert pandas dataframe to list\n",
    "i94port_list=i94port_df.values.tolist()\n",
    "\n",
    "#Convert list to spark dataframe\n",
    "i94port_type = StructType([StructField('id', StringType(), True),\n",
    "                            StructField('port_city', StringType(), True),\n",
    "                            StructField('port_state', StringType(), True)\n",
    "                            ])\n",
    "i94port = spark.createDataFrame(i94port_list, i94port_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+---------+----------+\n",
      "|   cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|port_city|port_state|\n",
      "+--------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+---------+----------+\n",
      "| 13351.0|2016.0|   4.0| 116.0| 116.0|    BGM|20545.0|    1.0|     ME|20547.0|  68.0|    1.0|  1.0|20160401|     DBL| null|      G|      O|   null|      M| 1948.0|09302016|     M|  null|    *GA| 9.250494923E10|MABIS|      B1|   BANGOR|        ME|\n",
      "| 26320.0|2016.0|   4.0| 131.0| 131.0|    BGM|20545.0|    1.0|     OH|20552.0|  46.0|    1.0|  1.0|20160401|     BEN| null|      G|      O|   null|      M| 1970.0|09302016|     M|  null|    *GA| 9.244752923E10|VPBMA|      B1|   BANGOR|        ME|\n",
      "| 51159.0|2016.0|   4.0| 148.0| 112.0|    BGM|20545.0|    1.0|     NE|20546.0|  49.0|    1.0|  1.0|20160401|     LSB| null|      G|      O|   null|      M| 1967.0|09302016|     M|  null|    *GA| 9.244742393E10|CSDFD|      B1|   BANGOR|        ME|\n",
      "| 51160.0|2016.0|   4.0| 148.0| 112.0|    BGM|20545.0|    1.0|     NE|20546.0|  44.0|    1.0|  1.0|20160401|     LSB| null|      G|      O|   null|      M| 1972.0|09302016|     M|  null|    *GA| 9.244676753E10|CSDFD|      B1|   BANGOR|        ME|\n",
      "|154968.0|2016.0|   4.0| 582.0| 582.0|    BGM|20545.0|    1.0|     ME|20546.0|  42.0|    1.0|  1.0|20160401|     JDD| null|      G|      R|   null|      M| 1974.0|09302016|     M|  null|    *GA| 9.248105093E10|XAJBA|      B1|   BANGOR|        ME|\n",
      "|155457.0|2016.0|   4.0| 582.0| 582.0|    BGM|20545.0|    1.0|     ME|20546.0|  42.0|    1.0|  1.0|20160401|     MEX| null|      G|      R|   null|      M| 1974.0|09302016|     M|  null|    *GA| 9.248138363E10|XAJBA|      B1|   BANGOR|        ME|\n",
      "|155458.0|2016.0|   4.0| 582.0| 582.0|    BGM|20545.0|    1.0|     ME|20546.0|  41.0|    1.0|  1.0|20160401|     MEX| null|      G|      R|   null|      M| 1975.0|09302016|     M|  null|    *GA| 9.248080703E10|XAJBA|      B1|   BANGOR|        ME|\n",
      "|155459.0|2016.0|   4.0| 582.0| 582.0|    BGM|20545.0|    1.0|     ME|20546.0|  39.0|    1.0|  1.0|20160401|     MEX| null|      G|      R|   null|      M| 1977.0|09302016|     M|  null|    *GA| 9.248158433E10|XAJBA|      B1|   BANGOR|        ME|\n",
      "|155460.0|2016.0|   4.0| 582.0| 582.0|    BGM|20545.0|    1.0|     ME|20546.0|  36.0|    1.0|  1.0|20160401|     MEX| null|      G|      R|   null|      M| 1980.0|09302016|     M|  null|    *GA| 9.248122403E10|XAJBA|      B1|   BANGOR|        ME|\n",
      "|155461.0|2016.0|   4.0| 582.0| 582.0|    BGM|20545.0|    1.0|     ME|20546.0|  29.0|    1.0|  1.0|20160401|     MEX| null|      G|      R|   null|      M| 1987.0|09302016|     M|  null|    *GA| 9.248463223E10|XAJBA|      B1|   BANGOR|        ME|\n",
      "|446492.0|2016.0|   4.0| 116.0| 116.0|    BGM|20547.0|    1.0|     GA|20551.0|  66.0|    2.0|  1.0|20160403|     DBL| null|      G|      R|   null|      M| 1950.0|10022016|     M|  null|    *GA| 9.268489463E10|MCELT|      B2|   BANGOR|        ME|\n",
      "|446493.0|2016.0|   4.0| 116.0| 116.0|    BGM|20547.0|    1.0|     GA|20551.0|  62.0|    2.0|  1.0|20160403|     DBL| null|      G|      R|   null|      M| 1954.0|10022016|     M|  null|    *GA| 9.268426023E10|MCELT|      B2|   BANGOR|        ME|\n",
      "|446494.0|2016.0|   4.0| 116.0| 116.0|    BGM|20547.0|    1.0|     GA|20551.0|  57.0|    2.0|  1.0|20160403|     DBL| null|      G|      R|   null|      M| 1959.0|10022016|     M|  null|    *GA| 9.268408243E10|MCELT|      B2|   BANGOR|        ME|\n",
      "|446495.0|2016.0|   4.0| 116.0| 116.0|    BGM|20547.0|    1.0|     GA|20551.0|  34.0|    2.0|  1.0|20160403|     DBL| null|      G|      R|   null|      M| 1982.0|10022016|     F|  null|    *GA| 9.268433453E10|MCELT|      B2|   BANGOR|        ME|\n",
      "|446496.0|2016.0|   4.0| 116.0| 116.0|    BGM|20547.0|    1.0|     GA|20551.0|  28.0|    2.0|  1.0|20160403|     DBL| null|      G|      R|   null|      M| 1988.0|10022016|     M|  null|    *GA| 9.268416593E10|MCELT|      B2|   BANGOR|        ME|\n",
      "|446610.0|2016.0|   4.0| 116.0| 116.0|    BGM|20547.0|    1.0|     GA|20551.0|  39.0|    1.0|  1.0|20160403|     LND| null|      G|      R|   null|      M| 1977.0|10022016|     M|  null|    *GA| 9.268396883E10|MCELT|      B1|   BANGOR|        ME|\n",
      "|452706.0|2016.0|   4.0| 124.0| 124.0|    BGM|20547.0|    1.0|     VA|20549.0|  38.0|    1.0|  1.0|20160403|     OSL| null|      G|      R|   null|      M| 1978.0|10022016|     M|  null|    *GA| 9.269255243E10|MBAEP|      B1|   BANGOR|        ME|\n",
      "|460085.0|2016.0|   4.0| 135.0| 135.0|    BGM|20547.0|    1.0|     FL|20550.0|  34.0|    2.0|  1.0|20160403|    null| null|      G|      R|   null|      M| 1982.0|07012016|     M|  null|    R0E|5.5526110933E10|EDC10|      WT|   BANGOR|        ME|\n",
      "|473180.0|2016.0|   4.0| 135.0| 135.0|    BGM|20547.0|    1.0|     NY|20687.0|  69.0|    1.0|  1.0|20160403|     BMB| null|      G|      I|   null|      M| 1947.0|10022016|     M|  null|    R0E| 9.266401103E10|EDC92|      B1|   BANGOR|        ME|\n",
      "|473264.0|2016.0|   4.0| 135.0| 135.0|    BGM|20547.0|    1.0|     NY|   null|  64.0|    1.0|  1.0|20160403|     JAK| null|      G|   null|   null|   null| 1952.0|10022016|     M|  null|    R0E| 9.266377643E10|EDC92|      B1|   BANGOR|        ME|\n",
      "+--------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Add columns from i94 port file to dataframe df_i94\n",
    "#def join(df_i94, i94port, df_i94.id == i94port.id, how='left'):\n",
    "    #df_i94 = df_i94.join(i94port, df_i94.id == i94port.id, how=how)\n",
    "    #repeated_columns = [c for c in df_i94.columns if c in i94port.colums]\n",
    "    #for col in repeated_columns:\n",
    "        #df_i94 = df_i94.drop(i94port[col])\n",
    "    #return df_i94\n",
    "\n",
    "#df_i94.show()\n",
    "    \n",
    "df_i94 = df_i94.join(i94port, df_i94.i94port == i94port.id, how = 'left')\n",
    "\n",
    "#Drop 'id' column\n",
    "df_i94 = df_i94.drop('id')\n",
    "df_i94.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----------------------------+---------+--------------------+--------+---------+-------+\n",
      "|        dt| AverageTemperature|AverageTemperatureUncertainty|     City|             Country|Latitude|Longitude|i94port|\n",
      "+----------+-------------------+-----------------------------+---------+--------------------+--------+---------+-------+\n",
      "|1852-07-01|             15.488|                        1.395|    Perth|           Australia|  31.35S|  114.97E|    PER|\n",
      "|1828-01-01|             -1.977|                        2.551|  Seattle|       United States|  47.42N|  121.97W|    SEA|\n",
      "|1743-11-01|              2.767|                        1.905| Hamilton|              Canada|  42.59N|   80.73W|    HAM|\n",
      "|1849-01-01|  7.399999999999999|                        2.699|  Ontario|       United States|  34.56N|  116.76W|    ONT|\n",
      "|1821-11-01|              2.322|                        2.375|  Spokane|       United States|  47.42N|  117.24W|    SPO|\n",
      "|1843-01-01| 18.874000000000002|                        2.017|Abu Dhabi|United Arab Emirates|  24.92N|   54.98E|    MAA|\n",
      "|1824-01-01|             25.229|                        1.094|    Anaco|           Venezuela|   8.84N|   64.05W|    ANA|\n",
      "|1855-05-01|              9.904|           1.4369999999999998|      Ica|                Peru|  13.66S|   75.14W|    CHI|\n",
      "|1835-01-01|              9.833|                        2.182|  Nogales|       United States|  31.35N|  111.20W|    NOG|\n",
      "|1743-11-01|  8.129999999999999|                        2.245|  Atlanta|       United States|  34.56N|   83.68W|    ATL|\n",
      "|1796-01-01|             15.552|                        2.305|      Mau|               India|  26.52N|   84.18E|    OGG|\n",
      "|1743-11-01|              3.264|                        1.665|   Newark|       United States|  40.99N|   74.56W|    NEW|\n",
      "|1857-01-01| 18.581000000000003|           1.8119999999999998|  Springs|        South Africa|  26.52S|   28.66E|    PSP|\n",
      "|1856-01-01| 26.055999999999997|           1.3769999999999998|      Ise|             Nigeria|   7.23N|    5.68E|    BOI|\n",
      "|1743-11-01|             18.722|                        2.302|  Orlando|       United States|  28.13N|   80.91W|    ORL|\n",
      "|1823-01-01|             11.602|           2.8160000000000003|   Laredo|       United States|  28.13N|   99.09W|    LCB|\n",
      "|1841-01-01| 13.107999999999999|                        2.519|     Tali|              Taiwan|  24.92N|  120.59E|    MET|\n",
      "|1828-01-01|-2.7630000000000003|                        2.617| Victoria|              Canada|  49.03N|  122.45W|    VIC|\n",
      "|1743-11-01| 1.1880000000000002|                        1.531|   Boston|       United States|  42.59N|   72.00W|    BOS|\n",
      "|1849-01-01|  8.091999999999999|           2.1919999999999997|Fairfield|       United States|  37.78N|  122.03W|    FTF|\n",
      "+----------+-------------------+-----------------------------+---------+--------------------+--------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add i94 port column to temperature dataset, correspond port code to city name\n",
    "@udf\n",
    "def port_city(city):\n",
    "    for key in i94ports:\n",
    "        if city.lower() in i94ports[key][0].lower():\n",
    "            return key\n",
    "        \n",
    "df_temperature = df_temperature.withColumn('i94port', port_city(df_temperature.City))\n",
    "\n",
    "    #Drop entries that have null values\n",
    "df_temperature = df_temperature.filter(df_temperature.i94port != 'null')\n",
    "\n",
    "df_temperature.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Extract columns to create immigration dimension table\n",
    "immigration_fact_table = df_i94.select(['i94yr', 'i94mon', 'i94cit', 'i94port', 'i94mode', 'i94bir', 'arrdate', 'depdate', 'i94visa'])\n",
    "\n",
    "#Write parquet files partitioned by i94port\n",
    "immigration_fact_table.write.mode('append').partitionBy('i94port').parquet('/tables/i94_dimension.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Extract columns to create temperature dimension table\n",
    "temperature_table = df_temperature.select('i94port', 'AverageTemperature', 'City', 'Country', 'Latitude', 'Longitude')\n",
    "temperature_table.write.mode('append').partitionBy('i94port').parquet('/tables/temp_dimension.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Extract columns to create airport code dimension table\n",
    "airport_code_table = airport_df.select('type', 'name', 'continent', 'iso_country', \n",
    "                                       'state', 'municipality', 'local_code', 'coordinates')\n",
    "airport_code_table.write.mode('append').partitionBy('state').parquet('/tables/airport_code.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Extract columns to create cities demographics table\n",
    "demographics_table = demographics_final.select('City', col('State Code').alias('state'), col('Median Age').alias('median_age'),\n",
    "                                               col('Male Population').alias('male_population'),\n",
    "                                               col('Female Population').alias('female_population'), \n",
    "                                               col('Total Population').alias('total_population'),\n",
    "                                               col('Foreign-born').alias('foreign_born'),\n",
    "                                               col('Average Household Size').alias('avg_household_size'),\n",
    "                                               'Asian', col('Black or African-American').alias('black'),\n",
    "                                               col('Hispanic or Latino').alias('hispanic'), 'White')\n",
    "demographics_table.write.mode('append').partitionBy('state').parquet('/tables/demographics.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create temporary SQL tables\n",
    "df_i94.createOrReplaceTempView('immigration_table')\n",
    "df_temperature.createOrReplaceTempView('temperature_table')\n",
    "\n",
    "#Create fact table\n",
    "immigration_fact_table = spark.sql(\"\"\"\n",
    "                                    SELECT i.i94yr AS year,\n",
    "                                        i.i94cit AS city,\n",
    "                                        i.i94port AS port,\n",
    "                                        i.i94mode AS travel_code,\n",
    "                                        i.i94visa AS immigration_reason,\n",
    "                                        i.i94mon AS month,\n",
    "                                        t.AverageTemperature AS temperature,\n",
    "                                        i.port_state\n",
    "                                    FROM immigration_table AS i\n",
    "                                    JOIN temperature_table AS t ON i.i94port = t.i94port    \n",
    "                                    \"\"\")\n",
    "\n",
    "#Write parquet file partitioned by i94port\n",
    "immigration_fact_table.write.mode('append').partitionBy('port_state', 'port').parquet('/tables/fact_table.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "Run Quality Checks\n",
    " * Count checks to ensure completeness\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data quality check\n",
    "def data_check(df,name):\n",
    "    table = df.count()\n",
    "    if table == 0:\n",
    "        print(\"Quality check failed because table {} contains {} records\".format(name, table))\n",
    "    else:\n",
    "        print(\"Quality check passed because table {} contains {} records\".format(name, table))\n",
    "        \n",
    "data_check(df_i94, \"i94 immigration table\")\n",
    "data_check(df_temperature, \"temperature table\")\n",
    "data_check(airport_df, \"airport code table\")\n",
    "data_check(demographics_final, \"demographics table\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Column name explanation from:\n",
    "\n",
    "1. I94 immigration data\n",
    "\n",
    "* i94yr: 4 digit year\n",
    "* i94mon: numeric month\n",
    "* i94cit: 3 digit code of origin city\n",
    "* i94port: 3 character code of destination city\n",
    "* i94mode: 1 digit travel code\n",
    "* i94bir: age of respondent in years\n",
    "* arrdate: arrival date\n",
    "* depdate: depature date\n",
    "* i94visa: reason for immigration\n",
    "\n",
    "2. Temperature data\n",
    "\n",
    "* i94port: 3 charater code of destination city\n",
    "* AverageTemperature: Average temperature\n",
    "* city: city name\n",
    "* country: country name\n",
    "* latitude: latitude\n",
    "* longtitude: longtitude\n",
    "\n",
    "3. Airport code data\n",
    "\n",
    "* type: type of aiport\n",
    "* name: airport name\n",
    "* continent: 2 character code of continent\n",
    "* iso_country: 2 character code of city\n",
    "* state: 2 character code of state\n",
    "* municipality: town name\n",
    "* local_code: local code\n",
    "* coordinates: coordinates\n",
    "\n",
    "4. U.S. Cities Demographics\n",
    "\n",
    "* city: city name\n",
    "* state: 2 character code of state\n",
    "* median_age: numeric age\n",
    "* male_population: numeric male population\n",
    "* female_population: numeric female population\n",
    "* total_population: numeric total population\n",
    "* foreign_born: numeric foreign born\n",
    "* avg_household_size: numeric average household size\n",
    "* asian: numeric count of Asian race\n",
    "* black: numeric count of Black or African-American race\n",
    "* hispanic: numeric count of Hispanic or Latino race\n",
    "* white: numeric count of White race"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "* Apache Spark is chosen for this project because it can quickly perform processing task on large dataset. \n",
    "\n",
    "* The data should be updated everytime the source data is updated. Other than that, the data should also be updated monthly. \n",
    "\n",
    "* Approach plan for possible scenarios:\n",
    "\n",
    " * The data was increased by 100x.\n",
    "     * Data will no longer be stored in local storage. I will store my data in AWS S3. In addtion, I will use AWS EMR to perform my ETL pipeline.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "     * I will use apache airflow to automate scheduling\n",
    " * The database needed to be accessed by 100+ people.\n",
    "     * I can upload my database to AWS Redshift cluster. There many users can have access to the database, and they will be able to perform data analysis queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
